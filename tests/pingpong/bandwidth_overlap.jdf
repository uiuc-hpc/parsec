extern "C" %{
/*
 * Copyright (c) 2019 The Universiy of Tennessee and The Universiy
 *                    of Tennessee Research Foundation. All rights
 *                    reserved.
 */

/* includes parsec headers */
#include <parsec.h>
#include <parsec/data_dist/matrix/two_dim_rectangle_cyclic.h>
#include <parsec/data_dist/matrix/matrix.h>
#include <parsec/vpmap.h>
#include <parsec/parsec_stats.h>

/* system and io */
#include <stdlib.h>
#include <stdio.h>
#include <string.h>
#include <math.h>
#include <time.h>
#include <stdalign.h>

#if defined(PARSEC_HAVE_MPI)
#include <mpi.h>
#elif defined(PARSEC_HAVE_LCI)
#include <lci.h>
#endif

typedef struct {
    alignas(64) double value;
} double64;

double64 **kernel_time;

static inline double timespec_to_ns(const struct timespec *ts)
{
    return (double)ts->tv_sec * 1e9 + (double)ts->tv_nsec;
}

static inline void kernel(double *restrict T, int size, int intensity, int t)
{
    for (int j = 0; j < intensity; j++)
        for (int i = 0; i < size; i++)
            T[i] = fma(T[i], (double)t, (double)j);
}

%}

descA       [ type = "parsec_tiled_matrix_dc_t*" ]
loops       [ type = "int" ]
frags       [ type = "int" ]
ws          [ type = "int" ]
size        [ type = "int" ]
intensity   [ type = "int" ]
sync        [ type = "int" ]
chains      [ type = "int" ]

ping_pong(t, f, c)
t = 0 .. loops-1
f = 0 .. frags-1
c = 0 .. chains-1
: descA((t+c)%ws, f)
RW  T <- (t == 0) ? descA((t+c)%ws, f)
      <- (t > 0 && sync == 0) ? T data_sync(t-1, f, c)
      <- (t > 0 && sync  > 0) ? T ping_pong(t-1, f, c)
      -> (sync == 0) ? T data_sync(t,   f, c)
      -> (sync  > 0) ? T ping_pong(t+1, f, c)
CTL C <- (sync > 0 && t >= sync) ? C exec_sync(t-sync, c)
      -> C exec_sync(t, c)
; loops-t
BODY
    struct timespec tstart, tend;
    double time;

    clock_gettime(CLOCK_MONOTONIC, &tstart);
    kernel(T, size, intensity, t);
    clock_gettime(CLOCK_MONOTONIC, &tend);

    time = timespec_to_ns(&tend) - timespec_to_ns(&tstart);
    kernel_time[es->virtual_process->vp_id][es->th_id].value += time;
END

/* If sync > 0, ensures that ping_pong(t+sync, 0 .. frags-1, c) cannot start
 * until all ping_pong(t, 0 .. frags-1, c) have completed execution
 * If sync == 0, ensures that data_sync(t, 0 .. frags-1, c) cannot start
 * until all ping_pong(t, 0 .. frags-1, c) have completed execution */
exec_sync(t, c)
t = 0 .. loops-1-sync
c = 0 .. chains-1
: descA((t+sync+c)%ws, 0)
CTL C <- C ping_pong(t, 0 .. frags-1, c)
      -> (sync == 0) ? C data_sync(t,      0 .. frags-1, c)
      -> (sync  > 0) ? C ping_pong(t+sync, 0 .. frags-1, c)
; loops-t
BODY
END

/* Ensure that data cannot be sent to ping_pong(t+1, f, c)
 * until all ping_pong(t, 0 .. frags-1, c) have completed execution */
data_sync(t, f, c)
t = 0 .. %{ return (sync > 0) ? -1 : loops-1; %}
f = 0 .. frags-1
c = 0 .. chains-1
: descA((t+c)%ws, f)
RO  T <- T ping_pong(t,   f, c)
      -> T ping_pong(t+1, f, c)
CTL C <- C exec_sync(t, c)
; loops-t
BODY
END

extern "C" %{

/**
 * @brief bandwidth,  no-blocking
 *
 * @param [in] dcA: data
 * @param [in] loops: loops of bandwidth
 * @param [in] frags: fragments
 * @param [in] ws: number of process
 * @param [in] size: number of doubles
 */
parsec_taskpool_t*
parsec_bandwidth_overlap_New(parsec_tiled_matrix_dc_t *dcA,
                    int loops, int frags, int ws,
                    int size, int intensity, int sync, int chains)
{
  parsec_taskpool_t* bandwidth_overlap_taskpool;
  parsec_bandwidth_overlap_taskpool_t* taskpool = NULL;

  if( loops < 1 || frags < 1 || size < 1 || chains < 1 ) {
    fprintf(stderr, "loops/frags/size/chains should not be less than 1\n");
    exit(1);
  }

  if( intensity < 0 || sync < 0 ) {
    fprintf(stderr, "intensity/sync should not be less than 0\n");
    exit(1);
  }

  taskpool = parsec_bandwidth_overlap_new(dcA, loops, frags, ws,
                                          size, intensity, sync, chains);
  bandwidth_overlap_taskpool = (parsec_taskpool_t*)taskpool;

  parsec_matrix_add2arena( &taskpool->arenas_datatypes[PARSEC_bandwidth_overlap_DEFAULT_ARENA],
                           parsec_datatype_double_t, matrix_UpperLower,
                           1, 1, size, 1,
                           PARSEC_ARENA_ALIGNMENT_SSE, -1 );

  return bandwidth_overlap_taskpool;
}

/**
 * @param [inout] the parsec object to destroy
*/
void parsec_bandwidth_overlap_Destruct(parsec_taskpool_t *taskpool)
{
  parsec_bandwidth_overlap_taskpool_t *bandwidth_overlap_taskpool = (parsec_bandwidth_overlap_taskpool_t *)taskpool;
  parsec_matrix_del2arena(&bandwidth_overlap_taskpool->arenas_datatypes[PARSEC_bandwidth_overlap_DEFAULT_ARENA]);
  parsec_taskpool_free(taskpool);
}

/**
 * @brief bandwidth
 *
 * @param [in] dcA: data
 * @param [in] loops: loops of bandwidth
 * @param [in] frags: fragments
 * @param [in] ws: number of process
 * @param [in] size: number of doubles
 */
int parsec_bandwidth_overlap(parsec_context_t *parsec,
                    parsec_tiled_matrix_dc_t *dcA,
                    int loops, int frags, int ws,
                    int size, int intensity, int sync, int chains)
{
  parsec_taskpool_t *parsec_bandwidth_overlap = NULL;

  parsec_bandwidth_overlap = parsec_bandwidth_overlap_New(dcA,
                              loops, frags, ws, size, intensity, sync, chains);

  if( parsec_bandwidth_overlap != NULL ){
      parsec_enqueue(parsec, parsec_bandwidth_overlap);
      parsec_context_start(parsec);
      parsec_context_wait(parsec);
      parsec_bandwidth_overlap_Destruct(parsec_bandwidth_overlap);
  }

  return 0;
}

typedef struct {
    double value;
    const char *prefix;
} SI_t;

static const char *SI_prefixes[] = {
    "q", "r", "y", "z", "a", "f", "p", "n", "Âµ", "m",
    " ",
    "k", "M", "G", "T", "P", "E", "Z", "Y", "R", "Q",
};

static inline SI_t to_SI(double value)
{
    double exponent = floor(log10(value)/3.0);
    if (exponent < -10.0)
        exponent = -10.0;
    if (exponent > 10.0)
        exponent = 10.0;
    size_t index = (size_t)(exponent + 10.0);
    return (SI_t){ value/pow(1000.0, exponent), SI_prefixes[index] };
}

#if defined(PARSEC_HAVE_LCI)
static void lci_sum_op(void *dst, const void *src, size_t count)
{
    double *d = dst;
    const double *s = src;
    *d += *s;
}
#endif /* PARSEC_HAVE_LCI */

int main(int argc, char *argv[])
{
    parsec_context_t* parsec;
    parsec_taskpool_t* bandwidth_overlap_taskpool;
    parsec_bandwidth_overlap_taskpool_t* taskpool = NULL;
    int rank, nodes, ch, i;
    int vp, th, th_arr_size;
    int pargc = 0, dashdash = -1;
    char **pargv;
    struct timespec tstart, tend;
    double t, exec, lat, rate, bw, flop, task, efficiency;
    size_t msize, ntasks, messages;
    SI_t si_lat, si_rate, si_bw, si_flop, si_flops, si_task;

    /* Default */
    int loops = 100;
    int frags = 60;
    int size = 1024;
    int cores = 1;
    int nb_runs = 1;
    int intensity = 1;
    int sync = 0;
    int chains = 1;

#if   defined(PARSEC_HAVE_MPI)
    {
        int provided;
        MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);
    }
    MPI_Comm_size(MPI_COMM_WORLD, &nodes);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
#elif defined(PARSEC_HAVE_LCI)
    LCI_initialize();
    nodes = LCI_NUM_PROCESSES;
    rank = LCI_RANK;
#else
    nodes = 1;
    rank = 0;
#endif

    while ((ch = getopt(argc, argv, "n:f:l:c:h:e:i:s:a:")) != -1) {
        switch (ch) {
            case 'n': loops = atoi(optarg); break;
            case 'f': frags = atoi(optarg); break;
            case 'l': size = atoi(optarg) / sizeof(double); break;
            case 'e': nb_runs = atoi(optarg); break;
            case 'c': cores = atoi(optarg); break;
            case 'i': intensity = atoi(optarg); break;
            case 's': sync = atoi(optarg); break;
            case 'a': chains = atoi(optarg); break;
            case '?': case 'h': default:
                fprintf(stderr,
                        "-n : loops of bandwidth(default: 100)\n"
                        "-f : frags, number of fragments (default: 60)\n"
                        "-l : size, size of message (default: 1024 * sizeof(double))\n"
                        "-i : intensity, number of flops per 8 bytes (default: 1)\n"
                        "-s : sync, loops ahead to sync with, 0 syncs data (default: 0)\n"
                        "-a : chainsets, sets of chains to start round-robin (default: 1)\n"
                        "-c : number of cores used (default: 1)\n"
                        "-e : number of runs (default: 1)\n"
                        "\n");
                 exit(1);
        }
    }

#if 0
    for(i = 1; i < argc; i++) {
        if( strcmp(argv[i], "--") == 0 ) {
            dashdash = i;
            pargc = 1;
        } else if( dashdash != -1 ) {
            pargc++;
        }
    }
    pargv = (char**)malloc( (pargc+2) * sizeof(char*));
    if( dashdash != -1 ) {
        pargv[0] = strdup(argv[0]);
        for(i = dashdash+1; i < argc; i++) {
            pargv[i-dashdash] = strdup(argv[i]);
        }
        pargv[i-dashdash] = NULL;
    } else {
        pargv[0] = NULL;
    }
#else
    pargc = argc - optind + 1;
    pargv = malloc((pargc+1) * sizeof(char*));
    pargv[0] = argv[0];
    for (i = 1; i < pargc; i++) {
        pargv[i] = argv[optind + i - 1];
    }
    pargv[pargc] = NULL;
#endif

    /* Initialize PaRSEC */
    parsec = parsec_init(cores, &pargc, &pargv);

    free(pargv);
    if( NULL == parsec ) {
        /* Failed to correctly initialize. In a correct scenario report
         * upstream, but in this particular case bail out.
         */
        exit(-1);
    }

    /* Get real number of cores */
    cores = vpmap_get_nb_total_threads();

    kernel_time = malloc(sizeof(*kernel_time) * vpmap_get_nb_vp());
    for(vp = 0; vp < vpmap_get_nb_vp(); vp++) {
        th_arr_size = sizeof(**kernel_time) * vpmap_get_nb_threads_in_vp(vp);
        kernel_time[vp] = aligned_alloc(64, th_arr_size);
    }

    /* initializing matrix structure */
    two_dim_block_cyclic_t dcA;
    two_dim_block_cyclic_init(&dcA, matrix_RealDouble, matrix_Tile,
                              nodes, rank, 1, size, nodes, size*frags, 0, 0,
                              nodes, size*frags, 1, 1, nodes);
    dcA.mat = parsec_data_allocate((size_t)dcA.super.nb_local_tiles *
                                   (size_t)dcA.super.bsiz *
                                   (size_t)parsec_datadist_getsizeoftype(dcA.super.mtype));
    parsec_data_collection_set_key((parsec_data_collection_t*)&dcA, "dcA");

    for(i = 0; i < nb_runs; i++) {
        /* bandwidth */
        taskpool = parsec_bandwidth_overlap_new((parsec_tiled_matrix_dc_t *)&dcA,
                                       loops, frags, nodes, size, intensity, sync, chains);

        bandwidth_overlap_taskpool = (parsec_taskpool_t*)taskpool;

        parsec_matrix_add2arena( &taskpool->arenas_datatypes[PARSEC_bandwidth_overlap_DEFAULT_ARENA],
                                 parsec_datatype_double_t, matrix_UpperLower,
                                 1, 1, size, 1,
                                 PARSEC_ARENA_ALIGNMENT_SSE, -1 );

        for(vp = 0; vp < vpmap_get_nb_vp(); vp++)
            for(th = 0; th < vpmap_get_nb_threads_in_vp(vp); th++)
                kernel_time[vp][th].value = 0.0;
#ifdef PARSEC_STATS_SCHED
        parsec_sched_stat_reset(parsec);
#endif
#ifdef PARSEC_STATS_COMM
        parsec_comm_stat_reset(parsec);
#endif

        /* Time start */
#if defined(PARSEC_HAVE_MPI)
        MPI_Barrier(MPI_COMM_WORLD);
#elif defined(PARSEC_HAVE_LCI)
        lci_barrier();
#endif  /* defined(PARSEC_HAVE_MPI) */

        clock_gettime(CLOCK_MONOTONIC, &tstart);

        parsec_context_add_taskpool(parsec, bandwidth_overlap_taskpool);
        parsec_context_start(parsec);
        parsec_context_wait(parsec);

        /* Time end */
#if defined(PARSEC_HAVE_MPI)
        MPI_Barrier(MPI_COMM_WORLD);
#elif defined(PARSEC_HAVE_LCI)
        lci_barrier();
#endif  /* defined(PARSEC_HAVE_MPI) */
        clock_gettime(CLOCK_MONOTONIC, &tend);

        exec = 0.0;
        for(vp = 0; vp < vpmap_get_nb_vp(); vp++)
            for(th = 0; th < vpmap_get_nb_threads_in_vp(vp); th++)
                exec += kernel_time[vp][th].value;
#if defined(PARSEC_HAVE_MPI)
        MPI_Allreduce(MPI_IN_PLACE, &exec, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
#elif defined(PARSEC_HAVE_LCI)
        lci_allreducem(&exec, sizeof(double), lci_sum_op);
#endif  /* defined(PARSEC_HAVE_MPI) */

#ifdef PARSEC_STATS_SCHED
        parsec_sched_stat_print(parsec);
#endif
#ifdef PARSEC_STATS_COMM
        parsec_comm_stat_print(parsec);
#endif

        if( 0 == rank ) {
            exec *= 1e-9;
            t = (timespec_to_ns(&tend) - timespec_to_ns(&tstart)) * 1e-9;
            msize    = sizeof(double[size]);
            ntasks   = (size_t)loops * (size_t)frags * (size_t)chains;
            messages = (size_t)(loops-1) * (size_t)frags * (size_t)chains;

            lat  = t / (double)(loops-1);
            rate = (double)messages / (t * (double)nodes);
            bw   = rate * (double)(msize * 8);
            flop = (double)(ntasks * 2 * (size_t)intensity * (size_t)size);
            task = exec / (double)ntasks;
            efficiency = exec / (t * (double)nodes * (double)cores);

            si_lat   = to_SI(lat);
            si_rate  = to_SI(rate);
            si_bw    = to_SI(bw);
            si_flop  = to_SI(flop);
            si_flops = to_SI(flop/t);
            si_task  = to_SI(task);
            efficiency *= 100.0;

            printf("%d %d %d %zu: %#08.4g s, %7.3f %ss, %7.3f %smsg/s, "
                   "%7.3f %sbit/s, %7.3f %sFLOP, %7.3f %sFLOP/s, "
                   "%7.3f %ss/task, %6.3f%% efficiency\n",
                   loops, frags, chains, msize, t,
                   si_lat.value,   si_lat.prefix,
                   si_rate.value,  si_rate.prefix,
                   si_bw.value,    si_bw.prefix,
                   si_flop.value,  si_flop.prefix,
                   si_flops.value, si_flops.prefix,
                   si_task.value,  si_task.prefix,
                   efficiency);
        }

        parsec_bandwidth_overlap_Destruct(bandwidth_overlap_taskpool);
    }

    for(vp = 0; vp < vpmap_get_nb_vp(); vp++)
        free(kernel_time[vp]);
    free(kernel_time);

    parsec_tiled_matrix_dc_destroy((parsec_tiled_matrix_dc_t*)&dcA);

    /* Clean up parsec*/
    parsec_fini(&parsec);

#if   defined(PARSEC_HAVE_MPI)
    MPI_Finalize();
#elif defined(PARSEC_HAVE_LCI)
    LCI_finalize();
#endif

    return 0;
}

%}
