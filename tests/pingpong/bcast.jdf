extern "C" %{
/*
 * Copyright (c) 2019 The Universiy of Tennessee and The Universiy
 *                    of Tennessee Research Foundation. All rights
 *                    reserved.
 */

/* includes parsec headers */
#include <parsec.h>
#include <parsec/data_dist/matrix/two_dim_rectangle_cyclic.h>
#include <parsec/data_dist/matrix/matrix.h>
#include <parsec/parsec_stats.h>

/* system and io */
#include <stdlib.h>
#include <stdio.h>
#include <string.h>

#if defined(PARSEC_HAVE_MPI)
#include <mpi.h>
#elif defined(PARSEC_HAVE_LCI)
#include <lci.h>
#endif

%}

descA       [ type = "parsec_tiled_matrix_dc_t*" ]
loops       [ type = "int" ]
frags       [ type = "int" ]
ws          [ type = "int" ]

ping_pong(r, f, t)
// Execution space
r = 0 .. ws-1
f = 0 .. frags-1
t = 0 .. loops
// Parallel partitioning
: descA(r, f)
// Data flows
READ T <- (t == 0) ? descA(r, f) : T ping_pong(r, f, t-1)
       -> T ping_pong(r, f, t+1)
       -> (r == ((t+f)%ws)) ? U ping_pong(0..ws-1, f, t+1)
READ U <- (t == 0) ? descA(r, f) : T ping_pong((t+f-1)%ws, f, t-1)
// Priority
; (loops - t) * ws + (ws - ((r - ((t+f)%ws) + ws)%ws))
BODY
END

extern "C" %{

/**
 * @brief bcast,  no-blocking
 *
 * @param [in] dcA: data
 * @param [in] loops: loops of bcast
 * @param [in] frags: fragments
 * @param [in] ws: number of process
 * @param [in] size: number of doubles
 */
parsec_taskpool_t*
parsec_bcast_New(parsec_tiled_matrix_dc_t *dcA,
                    int loops, int frags, int ws, int size)
{
  parsec_taskpool_t* bcast_taskpool;
  parsec_bcast_taskpool_t* taskpool = NULL;

  if( loops < 1 || frags < 1 || size < 1) {
    fprintf(stderr, "loops/frags/size should not smaller than 1\n");
    exit(1);
  }

  taskpool = parsec_bcast_new(dcA, loops, frags, ws);
  bcast_taskpool = (parsec_taskpool_t*)taskpool;

  parsec_matrix_add2arena( &taskpool->arenas_datatypes[PARSEC_bcast_DEFAULT_ARENA],
                           parsec_datatype_double_t, matrix_UpperLower,
                           1, 1, size, 1,
                           PARSEC_ARENA_ALIGNMENT_SSE, -1 );

  return bcast_taskpool;
}

/**
 * @param [inout] the parsec object to destroy
*/
void parsec_bcast_Destruct(parsec_taskpool_t *taskpool)
{
  parsec_bcast_taskpool_t *bcast_taskpool = (parsec_bcast_taskpool_t *)taskpool;
  parsec_matrix_del2arena(&bcast_taskpool->arenas_datatypes[PARSEC_bcast_DEFAULT_ARENA]);
  parsec_taskpool_free(taskpool);
}

/**
 * @brief bcast
 *
 * @param [in] dcA: data
 * @param [in] loops: loops of bcast
 * @param [in] frags: fragments
 * @param [in] ws: number of process
 * @param [in] size: number of doubles
 */
int parsec_bcast(parsec_context_t *parsec,
                    parsec_tiled_matrix_dc_t *dcA,
                    int loops, int frags, int ws, int size)
{
  parsec_taskpool_t *parsec_bcast = NULL;

  parsec_bcast = parsec_bcast_New(dcA, loops, frags, ws, size);

  if( parsec_bcast != NULL ){
      parsec_enqueue(parsec, parsec_bcast);
      parsec_context_start(parsec);
      parsec_context_wait(parsec);
      parsec_bcast_Destruct(parsec_bcast);
  }

  return 0;
}

int main(int argc, char *argv[])
{
    parsec_context_t* parsec;
    parsec_taskpool_t* bcast_taskpool;
    parsec_bcast_taskpool_t* taskpool = NULL;
    int rank, nodes, ch, i;
    int pargc = 0, dashdash = -1;
    char **pargv;
    struct timespec tstart, tend;
    double t, bw;

    /* Default */
    int loops = 100;
    int frags = 60;
    int size = 1024;
    int cores = 1;
    int nb_runs = 1;

#if   defined(PARSEC_HAVE_MPI)
    {
        int provided;
        MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);
    }
    MPI_Comm_size(MPI_COMM_WORLD, &nodes);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
#elif defined(PARSEC_HAVE_LCI)
    LCI_initialize();
    nodes = LCI_NUM_PROCESSES;
    rank = LCI_RANK;
#else
    nodes = 1;
    rank = 0;
#endif

    while ((ch = getopt(argc, argv, "n:f:l:c:h:e:")) != -1) {
        switch (ch) {
            case 'n': loops = atoi(optarg); break;
            case 'f': frags = atoi(optarg); break;
            case 'l': size = atoi(optarg) / sizeof(double); break;
            case 'e': nb_runs = atoi(optarg); break;
            case 'c': cores = atoi(optarg); break;
            case '?': case 'h': default:
                fprintf(stderr,
                        "-n : loops of bcast(default: 100)\n"
                        "-f : frags, number of fragments (default: 60)\n"
                        "-l : size, size of message (default: 1024 * sizeof(double))\n"
                        "-c : number of cores used (default: 1)\n"
                        "-e : number of runs (default: 1)\n"
                        "\n");
                 exit(1);
        }
    }

#if 0
    for(i = 1; i < argc; i++) {
        if( strcmp(argv[i], "--") == 0 ) {
            dashdash = i;
            pargc = 1;
        } else if( dashdash != -1 ) {
            pargc++;
        }
    }
    pargv = (char**)malloc( (pargc+2) * sizeof(char*));
    if( dashdash != -1 ) {
        pargv[0] = strdup(argv[0]);
        for(i = dashdash+1; i < argc; i++) {
            pargv[i-dashdash] = strdup(argv[i]);
        }
        pargv[i-dashdash] = NULL;
    } else {
        pargv[0] = NULL;
    }
#else
    pargc = argc - optind + 1;
    pargv = malloc((pargc+1) * sizeof(char*));
    pargv[0] = argv[0];
    for (i = 1; i < pargc; i++) {
        pargv[i] = argv[optind + i - 1];
    }
    pargv[pargc] = NULL;
#endif

    /* Initialize PaRSEC */
    parsec = parsec_init(cores, &pargc, &pargv);

    free(pargv);
    if( NULL == parsec ) {
        /* Failed to correctly initialize. In a correct scenario report
         * upstream, but in this particular case bail out.
         */
        exit(-1);
    }

    /* If the number of cores has not been defined as a parameter earlier
     * update it with the default parameter computed in parsec_init. */
    if(cores <= 0)
    {
        int p, nb_total_comp_threads = 0;
        for(p = 0; p < parsec->nb_vp; p++) {
            nb_total_comp_threads += parsec->virtual_processes[p]->nb_cores;
        }
        cores = nb_total_comp_threads;
    }

    /* initializing matrix structure */
    two_dim_block_cyclic_t dcA;
    two_dim_block_cyclic_init(&dcA, matrix_RealDouble, matrix_Tile,
                              nodes, rank, 1, size, nodes, size*frags, 0, 0,
                              nodes, size*frags, 1, 1, nodes);
    dcA.mat = parsec_data_allocate((size_t)dcA.super.nb_local_tiles *
                                   (size_t)dcA.super.bsiz *
                                   (size_t)parsec_datadist_getsizeoftype(dcA.super.mtype));
    parsec_data_collection_set_key((parsec_data_collection_t*)&dcA, "dcA");

    for(i = 0; i < nb_runs; i++) {
        /* bcast */
        taskpool = parsec_bcast_new((parsec_tiled_matrix_dc_t *)&dcA,
                                       loops, frags, nodes);

        bcast_taskpool = (parsec_taskpool_t*)taskpool;

        parsec_matrix_add2arena( &taskpool->arenas_datatypes[PARSEC_bcast_DEFAULT_ARENA],
                                 parsec_datatype_double_t, matrix_UpperLower,
                                 1, 1, size, 1,
                                 PARSEC_ARENA_ALIGNMENT_SSE, -1 );

#ifdef PARSEC_STATS_SCHED
        parsec_sched_stat_reset(parsec);
#endif
#ifdef PARSEC_STATS_COMM
        parsec_comm_stat_reset(parsec);
#endif

        /* Time start */
#if defined(PARSEC_HAVE_MPI)
        MPI_Barrier(MPI_COMM_WORLD);
#elif defined(PARSEC_HAVE_LCI)
        lci_barrier();
#endif  /* defined(PARSEC_HAVE_MPI) */
        clock_gettime(CLOCK_MONOTONIC, &tstart);

        parsec_context_add_taskpool(parsec, bcast_taskpool);
        parsec_context_start(parsec);
        parsec_context_wait(parsec);

        /* Time end */
#if defined(PARSEC_HAVE_MPI)
        MPI_Barrier(MPI_COMM_WORLD);
#elif defined(PARSEC_HAVE_LCI)
        lci_barrier();
#endif  /* defined(PARSEC_HAVE_MPI) */
        clock_gettime(CLOCK_MONOTONIC, &tend);

#ifdef PARSEC_STATS_SCHED
        parsec_sched_stat_print(parsec);
#endif
#ifdef PARSEC_STATS_COMM
        parsec_comm_stat_print(parsec);
#endif

        if( 0 == rank ) {
            t = (tend.tv_sec - tstart.tv_sec) * 1e9 + (tend.tv_nsec - tstart.tv_nsec);
            bw = ((double)(nodes - 1) * (double)loops * (double)frags * (double)size * sizeof(double) * 8) / t;
            printf("%d %d %d %lu %08.4g %4.8g Gbit/s\n", loops, frags, nodes, size*sizeof(double), t / 1e9, bw);
        }

        parsec_bcast_Destruct(bcast_taskpool);
    }

    parsec_tiled_matrix_dc_destroy((parsec_tiled_matrix_dc_t*)&dcA);

    /* Clean up parsec*/
    parsec_fini(&parsec);

#if   defined(PARSEC_HAVE_MPI)
    MPI_Finalize();
#elif defined(PARSEC_HAVE_LCI)
    LCI_finalize();
#endif

    return 0;
}

%}
